# Short Hunter Bot Optimization Patch
# Key improvements:
# 1. Cache instrument list to reduce API calls
# 2. Optimize candle fetching with batch processing
# 3. Add exponential backoff for API failures
# 4. Improve signal scoring algorithm
# 5. Reduce redundant computations

# Add caching class after imports:
class APICache:
    """Cache API responses to reduce calls"""
    def __init__(self, ttl_seconds: int = 300):
        self.cache = {}
        self.ttl = ttl_seconds  # 5 minutes default

    def get(self, key: str) -> Optional[any]:
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return data
        return None

    def set(self, key: str, value: any):
        self.cache[key] = (value, time.time())

    def clear(self):
        self.cache.clear()

# Add to global configuration:
# Add after existing imports
api_cache = APICache(ttl_seconds=300)  # 5 minute cache for instrument list

# Optimize get_instruments function:
async def get_instruments():
    """Fetch instrument list - OPTIMIZED with caching"""
    cache_key = "instruments_list"
    cached = api_cache.get(cache_key)
    if cached:
        return cached

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(OKX_INSTRUMENTS_URL, params={
                "instType": OKX_INST_TYPE,
                "uly": OKX_SETTLE
            }) as response:
                if response.status == 200:
                    data = await response.json()
                    instruments = [d['instId'] for d in data.get('data', [])]
                    api_cache.set(cache_key, instruments)
                    return instruments
    except Exception as e:
        logger.error(f"Failed to fetch instruments: {e}")
        return []

# Optimize fetch_candles with exponential backoff:
async def fetch_candles_with_backoff(
    symbol: str,
    limit: int = OKX_CANDLE_LIMIT,
    max_retries: int = 3
) -> List[List]:
    """Fetch candles with exponential backoff - OPTIMIZED"""
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(OKX_CANDLES_URL, params={
                    "instId": symbol,
                    "bar": OKX_CANDLE_BAR,
                    "limit": str(limit)
                }) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('data', [])
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"Failed to fetch candles for {symbol}: {e}")
                return []
            wait_time = (2 ** attempt) + random.random()
            logger.warning(f"API error (attempt {attempt + 1}/{max_retries}), retrying in {wait_time:.1f}s: {e}")
            await asyncio.sleep(wait_time)
    return []

# Optimize batch candle fetching:
async def fetch_candles_batch(symbols: List[str]) -> Dict[str, List[List]]:
    """Fetch candles for multiple symbols concurrently - OPTIMIZED"""
    tasks = {symbol: fetch_candles_with_backoff(symbol) for symbol in symbols}
    results = {}

    # Execute concurrently with semaphore to limit rate
    semaphore = asyncio.Semaphore(5)  # Max 5 concurrent requests

    async def fetch_with_limit(symbol):
        async with semaphore:
            return symbol, await tasks[symbol]

    tasks_with_limit = [fetch_with_limit(symbol) for symbol in symbols]
    completed = await asyncio.gather(*tasks_with_limit, return_exceptions=True)

    for result in completed:
        if isinstance(result, Exception):
            continue
        symbol, candles = result
        if candles:
            results[symbol] = candles

    return results

# Optimize signal calculation:
def calculate_signal_score(df: pd.DataFrame) -> int:
    """Calculate signal score - OPTIMIZED algorithm"""
    if len(df) < OKX_LOOKBACK:
        return 0

    score = 0
    reasons = []

    # Use numpy for faster calculations
    close = df['close'].values
    high = df['high'].values
    low = df['low'].values
    volume = df['volume'].values

    # Recent performance (last 20 candles)
    recent_close = close[-20:]

    # 1. Price action: Multiple rejection highs
    # Vectorized approach
    highs_recent = high[-20:]
    max_high = np.max(highs_recent)

    # Count rejections within 0.5% of recent high
    rejections = np.sum(highs_recent > max_high * 0.995)

    if rejections >= 3:
        score += 3
        reasons.append(f"{rejections} rejection highs (0.5% tolerance)")
    elif rejections >= 2:
        score += 2
        reasons.append(f"{rejections} rejection highs")
    elif rejections >= 1:
        score += 1
        reasons.append("Rejection high detected")

    # 2. Volume exhaustion
    vol_mean = np.mean(volume[-50:])
    vol_recent = volume[-20:]

    # Check for decreasing volume on highs
    vol_trend = np.polyfit(range(len(vol_recent)), vol_recent, 1)[0]

    if vol_trend < -vol_mean * 0.1:  # Volume decreasing > 10%
        score += 2
        reasons.append("Volume exhaustion on highs")

    # 3. RSI divergence (simplified check)
    # Fast RSI calculation
    delta = np.diff(close)
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)

    avg_gain = np.mean(gain[-14:])
    avg_loss = np.mean(loss[-14:])

    if avg_loss > 0:
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))

        if rsi > 70 and rsi < 75:  # Moderately overbought
            score += 1
            reasons.append(f"RSI overbought ({rsi:.1f})")
        elif rsi >= 75:  # Extremely overbought
            score += 2
            reasons.append(f"RSI extremely overbought ({rsi:.1f})")

    # 4. Price near resistance (vectorized)
    price_level = close[-1]

    # Find resistance levels using numpy
    highs_all = high[-96:]
    resistance_candidates = highs_all[highs_all > price_level * 1.02]

    if len(resistance_candidates) > 0:
        nearest_resistance = np.min(resistance_candidates)
        distance_pct = (nearest_resistance - price_level) / price_level * 100

        if distance_pct < 1.0:
            score += 2
            reasons.append(f"Within 1% of resistance (${nearest_resistance:.2f})")
        elif distance_pct < 2.0:
            score += 1
            reasons.append(f"Within 2% of resistance")

    # 5. Time-based scoring (scan at :45)
    current_min = datetime.now().minute
    if 40 <= current_min <= 50:
        score += 1  # Bonus for scanning near optimal time

    # 6. Volume spike detection
    vol_spike = volume[-1] / vol_mean
    if vol_spike > 2.0:
        score += 1
        reasons.append(f"Volume spike ({vol_spike:.1f}x)")

    # Maximum score cap
    return min(score, 10), reasons

# Optimize main scan loop:
async def scan_market():
    """Main scan loop - OPTIMIZED"""
    global market_data_cache

    logger.info("üîç Starting market scan...")

    # Check if it's scan time (:45 of each hour)
    now = datetime.now()
    if now.minute != SCAN_MINUTE:
        minutes_until = (SCAN_MINUTE - now.minute) % 60
        logger.info(f"‚è∞ Next scan in {minutes_until} minutes")
        return

    logger.info(f"üéØ Scan started at {now.strftime('%H:%M:%S')}")

    # Get instruments (cached)
    instruments = await get_instruments()
    if not instruments:
        logger.error("‚ùå No instruments to scan")
        return

    # Filter to tracked symbols only (performance optimization)
    tracked_symbols = [a['s'] for a in DEFAULT_ASSETS]
    symbols = [s for s in instruments if s in tracked_symbols]

    if not symbols:
        logger.warning("‚ö†Ô∏è No tracked symbols found")
        return

    logger.info(f"üìä Scanning {len(symbols)} symbols")

    # Batch fetch candles
    candles_dict = await fetch_candles_batch(symbols)

    signals_found = []
    for symbol, candles in candles_dict.items():
        if not candles or len(candles) < OKX_LOOKBACK:
            continue

        # Convert to DataFrame
        df = pd.DataFrame(
            candles,
            columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volume_ccy_quote', 'confirm']
        )
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

        # Calculate signal
        score, reasons = calculate_signal_score(df)

        if score >= 6:  # Minimum score threshold
            current_price = float(candles[0][4])

            # Calculate stop loss and take profit
            atr = calculate_atr(df, 14)
            stop_loss = current_price + (atr * 1.5)  # Short position
            take_profit = current_price - (atr * 2.0)  # Short position

            signal = Signal(
                symbol=symbol,
                price=current_price,
                score=score,
                reasons=reasons,
                stop_loss=stop_loss,
                take_profit=take_profit
            )

            signals_found.append(signal)
            logger.info(f"üéØ Signal found: {symbol} @ {current_price:.2f} (score: {score})")

    # Sort by score and send alerts
    signals_found.sort(key=lambda x: x.score, reverse=True)

    if signals_found:
        # Filter to top signals
        top_signals = signals_found[:3]

        for signal in top_signals:
            trade_tracker.add_trade(signal, trade_tracker.trade_counter + len(signals_found))
            send_discord_alert(signal)

        logger.info(f"‚úÖ Sent {len(top_signals)} signal(s)")
    else:
        logger.info("üì≠ No signals found in this scan")

# Add helper function for ATR:
def calculate_atr(df: pd.DataFrame, period: int = 14) -> float:
    """Calculate ATR - OPTIMIZED with numpy"""
    if len(df) < period + 1:
        return 0.0

    high = df['high'].values
    low = df['low'].values
    close = df['close'].values

    tr1 = high[1:] - low[1:]
    tr2 = np.abs(high[1:] - close[:-1])
    tr3 = np.abs(low[1:] - close[:-1])

    tr = np.maximum(tr1, np.maximum(tr2, tr3))
    return float(np.mean(tr[-period:]))
